# Limitations & Caveats

This document outlines the limitations, caveats, and constraints of this research. Understanding these limitations is essential for correctly interpreting the findings.

---

## High Severity

### MCP Uses Simulated Tools

**Impact:** High

The MCP-Enhanced agent calls simulated tool functions, not real Claude API tool-use. Results demonstrate the architectural pattern but not production performance.

**Mitigation:** MCP findings are clearly labeled with asterisks (*) throughout all documentation. The results show architectural potential, not production benchmark data.

**What This Means:** If you're evaluating MCP for production use, you'll need to run your own benchmarks with real tool integrations.

---

## Medium Severity

### A2A Calibration on Edge Cases

**Impact:** Medium

A2A Debate (with Sonnet) shows **dangerous overconfidence** (95%) on impossible tasks—recommending nonexistent products when it should decline. This is a calibration failure, not a strength.

**Mitigation:**
- We exclude Edge Case Testing from primary scenario averages
- The "Top Performer" metric uses only the 4 primary scenarios
- A2A's weakness is clearly documented throughout findings

**What This Means:** If you deploy A2A in production, implement additional validation for edge cases. Consider using Opus (well-calibrated at 10-18%) for high-stakes decisions, or add explicit impossibility detection before A2A debate.

---

### LLM Non-Determinism

**Impact:** Medium

Even with identical prompts and temperature settings, LLM outputs can vary between runs. The same scenario may produce different recommendations on repeated execution.

**Mitigation:** Focus on aggregate patterns rather than individual results. Architectural differences (A2A > Single for trade-offs) are more reliable than single-run comparisons.

**What This Means:** If you run the benchmarks yourself, expect some variation in specific outputs while overall patterns should remain consistent.

---

### Prompt Sensitivity

**Impact:** Medium

Small changes in prompt wording, formatting, or structure can significantly affect agent outputs and confidence scores.

**Mitigation:** Prompts were held constant across all benchmark runs. Results reflect these specific prompt formulations.

**What This Means:** Different prompt engineering could yield different results. Our findings apply to our prompt design.

---

### Sample Size

**Impact:** Medium

60 runs across 60 combinations (4 agents × 5 scenarios × 3 models) provides 1 sample per configuration.

**Mitigation:** Each recording captures actual Claude API behavior. Focus on patterns across agent types rather than statistical significance.

**What This Means:** Results represent real Claude reasoning but lack statistical power for narrow confidence intervals.

---

### Self-Reported Confidence

**Impact:** Medium

Confidence scores are generated by the agents themselves, not validated against ground truth product matches or human evaluation.

**Mitigation:** Treat confidence as a relative signal, not an absolute measure. Compare across architectures rather than to external benchmarks.

**What This Means:** "85% confidence" doesn't mean "85% probability of being correct." It means the agent felt more certain than when it reports 50%.

---

### E-commerce Domain Only

**Impact:** Medium

All testing focused on product recommendations. Findings may not generalize to other agent applications (code generation, research, creative writing).

**Mitigation:** Architectural insights (debate > single for trade-offs) likely transfer. Quantitative benchmarks are domain-specific.

**What This Means:** Apply findings to e-commerce and similar recommendation tasks with confidence. Extrapolate to other domains with caution.

---

## Low Severity

### Latency Variability

**Impact:** Low

API response times vary based on server load, network conditions, and Claude API capacity. Timing metrics may differ between sessions.

**Mitigation:** Relative latency patterns (Single faster than A2A) are more meaningful than absolute times.

**What This Means:** Don't rely on exact millisecond timings. Use latency data for relative comparisons.

---

### Static Product Catalog

**Impact:** Low

60 curated products may not represent the full complexity of real e-commerce catalogs with thousands of items.

**Mitigation:** Catalog was designed to test specific scenarios (trade-offs, impossible constraints) rather than scale performance.

**What This Means:** Results show agent reasoning quality, not ability to search large catalogs.

---

## Additional Considerations

### No Real Transactions

All recommendations are simulated. No actual purchases were made or validated.

### No Human Evaluation

Recommendations were not evaluated by human judges for quality or appropriateness.

### Single Testing Period

All benchmarks were run January 3-7, 2026. Model behavior may change with updates.

### API Pricing

Cost calculations use January 2026 pricing. API costs may change.

---

## What We Can Confidently Claim

Despite limitations, these findings are robust:

| Claim | Confidence |
|-------|------------|
| A2A outperforms Single on trade-off scenarios | High |
| Multi-Agent improves complex analysis | High |
| Single Agent handles impossible tasks gracefully | High |
| Opus shows better calibration than Sonnet on edge cases | High |
| Architecture choice matters more than model for many tasks | Medium-High |
| MCP pattern shows promise for data-driven decisions | Medium (simulated) |

---

## What Requires Further Research

| Question | Status |
|----------|--------|
| How do results scale to larger catalogs? | Not tested |
| Do patterns hold for non-e-commerce domains? | Not tested |
| What's the optimal prompt design per architecture? | Not explored |
| How do real MCP tools compare to simulated? | Not tested |
| Does statistical significance hold over 100+ runs? | Not tested |

---

## Recommendations for Practitioners

1. **Use findings directionally** - "A2A is better for trade-offs" is more reliable than "A2A achieves exactly 95.5%"

2. **Run your own benchmarks** - These results provide starting points, not definitive answers for your use case

3. **Test edge cases** - Include impossible/adversarial scenarios in your own testing

4. **Consider calibration** - Confidence scores matter for production systems

5. **Match architecture to task** - The main finding (architecture > model) is robust

---

## Future Work

If extending this research, consider:

- Multiple iterations per combination (N=3-5)
- Human evaluation of recommendations
- Real MCP tool integration
- Non-e-commerce domains
- Larger product catalogs
- Different prompt formulations
- Newer model versions as released
